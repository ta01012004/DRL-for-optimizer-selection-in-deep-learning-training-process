{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":15444,"sourceType":"datasetVersion","datasetId":11102},{"sourceId":714968,"sourceType":"datasetVersion","datasetId":366471},{"sourceId":3032274,"sourceType":"datasetVersion","datasetId":1856924}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\nimport torchvision.transforms as T\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, random_split\nimport numpy as np, time, random, os\nfrom collections import namedtuple\nfrom sklearn.metrics import precision_score, f1_score\nimport matplotlib.pyplot as plt\nfrom torchvision.models import ResNet18_Weights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:54:01.802430Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Mô hình CNN\n","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torchvision.models as models\n\ndef get_resnet18_scratch(num_classes: int = 10, freeze_backbone: bool = False):\n    \"\"\"\n    Trả về ResNet-18 khởi tạo ngẫu nhiên (weights=None).\n    - num_classes      : số lớp đầu ra.\n    - freeze_backbone  : True → chỉ huấn luyện lớp FC cuối.\n    \"\"\"\n    # 1. Tạo ResNet-18 khởi tạo ngẫu nhiên\n    model = models.resnet18(weights=None)        # <-- KHÔNG tải trọng số cũ\n\n    # 2. Thay thế fully-connected\n    in_feats = model.fc.in_features\n    model.fc = nn.Linear(in_feats, num_classes)\n\n    # 3. (Tuỳ chọn) đóng băng backbone\n    if freeze_backbone:\n        for p in model.parameters():\n            p.requires_grad = False\n        for p in model.fc.parameters():          \n            p.requires_grad = True\n\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = get_resnet18_scratch(\n    num_classes=10,\n    freeze_backbone=False   # huấn luyện toàn bộ mạng\n).to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## MẠNG PPO ","metadata":{}},{"cell_type":"code","source":"STATE_DIM = 6                     # acc, loss, Δacc, Δloss, epoch%, prev_action\nACTION_DIM = 2                    # 0 = SGD, 1 = SAM\n\nclass PolicyNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(STATE_DIM, 64), nn.ReLU(),\n            nn.Linear(64, 64), nn.ReLU(),\n            nn.Linear(64, ACTION_DIM), nn.Softmax(dim=-1),\n        )\n    def forward(self, x): return self.net(x)\n\nclass ValueNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(STATE_DIM, 64), nn.ReLU(),\n            nn.Linear(64, 64), nn.ReLU(),\n            nn.Linear(64, 1),\n        )\n    def forward(self, x): return self.net(x)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## #                               SAM Optimizer                                 #\n","metadata":{}},{"cell_type":"code","source":"class SAM(optim.Optimizer):\n    \"\"\"Sharpness-Aware Minimization wrapper (một bước trước + một bước sau)\"\"\"\n    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n        defaults = dict(rho=rho, **kwargs)\n        super().__init__(params, defaults)\n        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n        self.param_groups = self.base_optimizer.param_groups\n\n    @torch.no_grad()\n    def first_step(self, zero_grad=False):\n        grad_norm = torch.norm(torch.stack([\n            p.grad.norm() for group in self.param_groups\n            for p in group['params'] if p.grad is not None\n        ]))\n        scale = [group['rho'] / (grad_norm + 1e-12) for group in self.param_groups]\n        for group, s in zip(self.param_groups, scale):\n            for p in group['params']:\n                if p.grad is None: continue\n                e_w = p.grad * s.to(p)\n                p.add_(e_w);  self.state[p]['e_w'] = e_w\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def second_step(self, zero_grad=False):\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None: continue\n                p.sub_(self.state[p]['e_w'])\n        self.base_optimizer.step()\n        if zero_grad: self.zero_grad()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# #                       PPO TRỢ GIÚP: ADV, RETURNS, BUFFER                    #\n","metadata":{}},{"cell_type":"markdown","source":"Hàm gae_returns có nhiệm vụ từ một chuỗi các “trải nghiệm” (Transition) tính ra hai thứ:\n\nreturns (còn gọi là target value)\n\nadvs (advantage estimate, dùng để cập nhật chính sách)","metadata":{}},{"cell_type":"code","source":"Transition = namedtuple('Transition', ['state','action','logp_old','reward','value','entropy'])\n\ndef gae_returns(trans, gamma=0.99, lam=0.95):\n    rewards  = [t.reward for t in trans]\n    values   = [t.value  for t in trans] + [0.0]\n    returns, advs = [], []\n    gae = 0\n    for i in reversed(range(len(trans))):\n        delta = rewards[i] + gamma*values[i+1] - values[i]\n        gae   = delta + gamma*lam*gae\n        advs.insert(0, gae)\n        returns.insert(0, gae + values[i])\n    advs = torch.tensor(advs,  dtype=torch.float32)\n    advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n    returns = torch.tensor(returns, dtype=torch.float32)\n    return returns, advs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# #                                TRAIN LOOP                                   #\n","metadata":{}},{"cell_type":"code","source":"from torchvision.datasets import CIFAR10\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, random_split\nimport torch, torch.nn.functional as F, time, numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score\nimport matplotlib.pyplot as plt\n\ndef train():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    #  Data ---------------------------------------------------------------\n    tf_train = T.Compose([\n        T.RandomCrop(32, padding=4),\n        T.RandomHorizontalFlip(),\n        T.ToTensor(),\n        T.Normalize((0.4914, 0.4822, 0.4465),\n                    (0.2023, 0.1994, 0.2010)),\n    ])\n    tf_eval = T.Compose([\n        T.ToTensor(),\n        T.Normalize((0.4914, 0.4822, 0.4465),\n                    (0.2023, 0.1994, 0.2010)),\n    ])\n\n    full_train = CIFAR10(root='./data', train=True, download=True, transform=tf_train)\n    test_set   = CIFAR10(root='./data', train=False, download=True, transform=tf_eval)\n\n    n_val = 5_000\n    train_ds, val_ds = random_split(full_train, [len(full_train) - n_val, n_val])\n\n    dl_train = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n    dl_val   = DataLoader(val_ds,   batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n    dl_test  = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n\n    print(f\"Dataset → Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_set)}\")\n\n    #  Model & PPO nets ---------------------------------------------------\n    import torchvision.models as models\n    model       = ResNet(models.resnet18(weights=None)).to(device)\n    policy_net  = PolicyNet().to(device)\n    value_net   = ValueNet().to(device)\n    opt_policy  = torch.optim.Adam(policy_net.parameters(), lr=3e-4)\n    opt_value   = torch.optim.Adam(value_net.parameters(),  lr=1e-3)\n\n    eps_clip, ent_coef, vf_coef = 0.2, 1e-2, 0.5\n    num_epochs, rollouts_per_ep, ppo_epochs = 100, 4, 4\n    best_val, patience, patience_cnt = 0, 10, 0\n\n    history = {k: [] for k in\n               ['train_acc', 'train_loss', 'val_acc', 'val_loss',\n                'test_acc', 'test_loss', 'reward', 'action']}\n\n    # Khởi tạo các biến theo dõi\n    prev_acc = prev_loss = 0.0\n    prev_val_acc = prev_val_loss = 0.0\n    prev_act = 0  # 0=SGD, 1=SAM\n\n    # Hàm đánh giá nhanh ----------------------------------------------------\n    def eval_loader(loader):\n        model.eval()\n        tot_loss = correct = total = 0\n        with torch.no_grad():\n            for x, y in loader:\n                x, y = x.to(device), y.to(device)\n                logits = model(x)\n                loss   = F.cross_entropy(logits, y)\n                tot_loss += loss.item() * x.size(0)\n                correct  += logits.argmax(1).eq(y).sum().item()\n                total    += x.size(0)\n        return correct / total, tot_loss / total\n\n    #  Epoch loop ---------------------------------------------------------\n    for ep in range(num_epochs):\n        t_ep = time.time()\n        buffer = []\n\n        for r in range(rollouts_per_ep):\n            t_roll = time.time()\n\n            # ---- build state ------------------------------------------------\n            delta_acc  = prev_acc      - history['train_acc'][-1] if history['train_acc'] else 0.0\n            delta_loss = prev_loss     - prev_loss               # =0 lần đầu\n            state_arr  = [prev_acc, prev_loss, delta_acc, delta_loss,\n                          ep / num_epochs, float(prev_act)]\n            state = torch.tensor(state_arr, dtype=torch.float32, device=device)\n\n            # ---- choose action ---------------------------------------------\n            with torch.no_grad():\n                dist = policy_net(state)\n            act = torch.distributions.Categorical(dist).sample()\n            logp_old = torch.log(dist[act])\n            entropy  = -(dist * torch.log(dist + 1e-8)).sum()\n\n            # ---- pick optimizer -------------------------------------------\n            if act.item() == 0:  # SGD\n                opt_base = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n                two_step = False\n            else:                # SAM\n                opt_base = SAM(model.parameters(), torch.optim.SGD, lr=0.001, momentum=0.9)\n                two_step = True\n\n            # ---- train one pass -------------------------------------------\n            model.train()\n            tot_loss = correct = total = 0\n            for x, y in dl_train:\n                x, y = x.to(device), y.to(device)\n                opt_base.zero_grad()\n\n                logits = model(x)\n                loss   = F.cross_entropy(logits, y)\n                loss.backward()\n\n                if two_step:\n                    opt_base.first_step(zero_grad=True)\n\n                    # forward + backward lần 2\n                    logits2 = model(x)\n                    loss2 = F.cross_entropy(logits2, y)\n                    loss2.backward()\n                    opt_base.second_step()\n                else:\n                    opt_base.step()\n\n                tot_loss += loss.item() * x.size(0)\n                correct  += logits.argmax(1).eq(y).sum().item()\n                total    += x.size(0)\n\n            train_acc  = correct / total\n            train_loss = tot_loss / total\n\n            # ---- validation ngay sau train ---------------------------------F\n            val_acc, val_loss = eval_loader(dl_val)\n\n            # ---- reward (dựa validation) ----------------------------------\n            delta_vacc  = val_acc  - prev_val_acc\n            delta_vloss = prev_val_loss - val_loss  # giảm loss => dương\n            raw_reward  = 10 * delta_vacc + delta_vloss - 0.05 * (act.item() != prev_act)\n            reward      = float(torch.clamp(torch.tensor(raw_reward), -10.0, 10.0))\n\n            # ---- lưu transition -------------------------------------------\n            value = value_net(state).item()\n            buffer.append(Transition(state, act, logp_old, reward, value, entropy))\n\n            # ---- cập nhật \"prev\" ------------------------------------------\n            prev_acc, prev_loss, prev_act = train_acc, train_loss, act.item()\n            prev_val_acc, prev_val_loss   = val_acc,  val_loss\n\n            # ---- log rollout ----------------------------------------------\n            print(f\"Ep {ep:<3} | Roll {r+1}: \"\n                  f\"Act={'SAM' if act.item() else 'SGD'} | \"\n                  f\"Rwd={reward:+6.2f} | \"\n                  f\"Time={time.time()-t_roll:5.2f}s | \"\n                  f\"Train L={train_loss:.4f}, Acc={train_acc*100:5.2f}% | \"\n                  f\"Val L={val_loss:.4f}, Acc={val_acc*100:5.2f}%\")\n\n        # ----- PPO update ---------------------------------------------------\n        returns, advs = gae_returns(buffer)\n        returns, advs = returns.to(device), advs.to(device)\n\n        S   = torch.stack([t.state   for t in buffer]).to(device)\n        A   = torch.tensor([t.action.item() for t in buffer],\n                           dtype=torch.long, device=device)\n        logp_old = torch.stack([t.logp_old for t in buffer]).to(device)\n        Ent      = torch.stack([t.entropy  for t in buffer]).to(device)\n\n        for _ in range(ppo_epochs):\n            dist = policy_net(S)\n            logp = torch.log(dist.gather(1, A.unsqueeze(1)).squeeze(1) + 1e-8)\n            ratio = torch.exp(logp - logp_old)\n\n            surr1 = ratio * advs\n            surr2 = torch.clamp(ratio, 1 - eps_clip, 1 + eps_clip) * advs\n            policy_loss = -torch.min(surr1, surr2).mean() - ent_coef * Ent.mean()\n\n            value_pred  = value_net(S).squeeze(1)\n            value_loss  = F.mse_loss(value_pred, returns)\n\n            opt_policy.zero_grad(); policy_loss.backward(); opt_policy.step()\n            opt_value.zero_grad();  (vf_coef * value_loss).backward(); opt_value.step()\n\n        # ----- log cuối epoch ----------------------------------------------\n        test_acc, test_loss = eval_loader(dl_test)\n\n        history['train_acc'].append(train_acc);  history['train_loss'].append(train_loss)\n        history['val_acc'].append(val_acc);      history['val_loss'].append(val_loss)\n        history['test_acc'].append(test_acc);    history['test_loss'].append(test_loss)\n        history['reward'].append(np.mean([t.reward for t in buffer]))\n        history['action'].append(prev_act)\n\n        print(f\"[Ep {ep:3}] TrainAcc={train_acc:.4f} | ValAcc={val_acc:.4f} | \"\n              f\"TestAcc={test_acc:.4f} | MeanRwd={history['reward'][-1]:+.2f} | \"\n              f\"Opt={'SAM' if prev_act else 'SGD'} | Δt={time.time()-t_ep:.1f}s\")\n\n        # Early-stopping\n        if val_acc > best_val:\n            best_val, patience_cnt = val_acc, 0\n        else:\n            patience_cnt += 1\n            if patience_cnt >= patience:\n                print(f\"Early-stop tại epoch {ep} – không cải thiện {patience} epoch.\")\n                break\n\n    # 4️⃣ Plot --------------------------------------------------------------\n    epochs = range(1, len(history['train_acc']) + 1)\n    plt.figure(figsize=(10,4))\n    plt.plot(epochs, history['train_acc'], label='Train')\n    plt.plot(epochs, history['val_acc'],   label='Val')\n    plt.plot(epochs, history['test_acc'],  label='Test')\n    plt.title('Accuracy'); plt.xlabel('Epoch'); plt.ylabel('Acc'); plt.legend(); plt.grid(); plt.show()\n\n    plt.figure(figsize=(10,4))\n    plt.plot(epochs, history['train_loss'], label='Train')\n    plt.plot(epochs, history['val_loss'],   label='Val')\n    plt.plot(epochs, history['test_loss'],  label='Test')\n    plt.title('Loss'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(); plt.show()\n\n    # 5️⃣ Final test metrics -------------------------------------------------\n    model.eval(); preds_all, labels_all = [], []\n    with torch.no_grad():\n        for x, y in dl_test:\n            x, y = x.to(device), y.to(device)\n            preds_all.append(model(x).argmax(1).cpu())\n            labels_all.append(y.cpu())\n    preds_all  = torch.cat(preds_all)\n    labels_all = torch.cat(labels_all)\n\n    print(\"\\n===== FINAL TEST METRICS =====\")\n    print(f\"Accuracy : {accuracy_score(labels_all, preds_all):.4f}\")\n    print(f\"F1 score : {f1_score(labels_all, preds_all, average='weighted'):.4f}\")\n    print(f\"Precision: {precision_score(labels_all, preds_all, average='weighted'):.4f}\")\nif __name__ == '__main__':\n    train() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n    train() ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}